\section{Experiments}\label{sec:experiments}

TODO introduction

\subsection{Setup}

\subsubsection{Hardware and Software}

We run our experiments on machines of the type \texttt{diamond} of the Scilens cluster owned by the CWI Database Architecture research
group.
These machines feature 4 Intel Xeon E5-4657Lv2 processors with 12 cores each and hyperthreading of 2 (48 cores / 96 threads)
Each core has 32 KB of 1st level cache, 32KB 2nd level cache.
The 3rd level cache are 30 MB shared between 12 cores.
The main memory consists of 1 TB of RAM DDR-3 memory.

The machines run a Fedora version 30 Linux system with the 5.0.17-300.fc30.x86\_64 kernel.
We use Spark 2.4.0 with Scala 2.11.12 on Java openJDK 1.8.
In the majority of our experiments, we use Spark in its standard configuration with enabled code generation.
We also tune the parameters for driver and executor memory usage (\texttt{spark.driver.memory} and \texttt{spark.executor.memory}) to fit
all necessary data into main memory.

\subsubsection{Algorithms}

In our experiments we use 4 different join algorithms.
Two of them are worst-case optimal joins.
That is our Leapfrog Triejoin implementation, \textit{LFTJ}, and a graph-pattern matching
specialized Leapfrog Triejoin developed in this thesis: \textit{Graph\textsc{WCOJ}}.
\textit{LFTJ} is only run as sequential algorithm as a baseline against \textit{GraphWCOJ}.
We compare these to algorithms in \cref{subsec:lftj-vs-graphWCOJ}.

The other two algorithms are Spark's versions of \textit{BroadcastHashJoin} and \textit{SortmergeJoin}.
We compare them against the sequential version of \textit{LFTJ} and \textit{GraphWCOJ} in \cref{subsec:seq-vs-spar}
and their scaling with \textit{GraphWCOJ} in \cref{subsec:graphWCOJ-vs-spark}.
We adjust the \texttt{"spark.sql.autoBroadcastJoinThreshold"} parameter to control if Spark is using a \texttt{BroadcastHashJoin} or a \texttt{SortMergeJoin}.

\subsubsection{Datasets}
We run the majority of our experiments on two datasets from different use-cases, social networks and product co-purchase.
We motivate our choice in the next paragraph.
\Cref{table:datasets} includes a list of all graph datasets mentioned throughout the thesis.

\begin{table}[]
    \centering
    \begin{tabular}{llrrl} \toprule
        Name    & Variant              &  Vertices   & Edges          & Source          \\ \midrule
        \textbf{SNB}         & sf1     &             &  453.032       & \cite{snb}      \\
        \textbf{Amazon}      & 0302    & 262,111     &  1,234,877     & \cite{snapnets} \\
                             & 0601    & 403,394     &  3,387,388     & \cite{snapnets} \\
        \textbf{Twitter}     & sc-d    & 81,306      &  1,768,135     & \cite{snapnets} \\
                             & sc-u    &   TODO      &       TODO     & \cite{snapnets} \\
        \textbf{LiveJournal} &         & 4,847,571   & 68,993,773     & \cite{snapnets} \\
        \textbf{Orkut}       &         & 3,072,441   & 117,185,083    & \cite{snapnets} \\ \bottomrule
    \end{tabular}
    \caption{A summary of all datasets mentioned in the thesis.
    Explanation of them and for the variants is given in running text.
    }
    \label{table:datasets}
\end{table}

 TODO add Vertices and edges numbers

The SNB benchmark~\cite{snb} generates data emulating the posts, messages and friendships in a social network.
For our experiments, we only use the friendships relationship (\texttt{person\_knows\_person.csv}) which is an undirected relationship.
After generation only edges of the kind \textit{src $<$ dst} exist, we generate the opposing edges before loading the dataset, such
that the edge table becomes truly undirected.
The benchmark comes with an extensively parameterizable graph generation engine
which allows us to experiment with sizes as small as 1GB and up to 1TB for big experiments and different levels of selectivity.
The different sizes are called scale-factor or \texttt{sf}, e.g. \texttt{SNB-sf1} refers to a Social network benchmark dataset generated with
default parameters and scale-factor 1.
We include the exact parameter used for generation in our repository under \texttt{experiments/snb/params.txt}. % CODEREF TODO include

The Amazon co-purchasing network contains edges between products that have been purchased together and hence are closely related to each other~\cite{snapnets}.
This is a directed relationship from the product purchased first to the product purchased second, both directions of an edge can exist if the order in which
products have been purchased varies.
The Snap dataset collection contains multiple Amazon co-purchase datasets, each of them containing a single day of purchases.
We choose the smallest and biggest dataset from the 2nd of March and the 1st of June 2003, we call them \texttt{Amazon-0302} and
\texttt{Amazon-0601}.
We pick co-purchase datasets for evaluation because former work often concentrated on social networks and web crawl based
graphs~\cite{myria-detailed,ammar2018distributed} but~\cite{salihoglu2018} points out that the biggest graphs are actually graphs like
the aforementioned Amazon graph containing purchase information.

To allow comparisons with former work, we run a subset of our experiments on the Twitter social circle network from~\cite{snapnets}.
This dataset includes the follower relationship of one thousand Twitter users; each of these follows 10 to 4.964 other users and
relationships between these are included.
The graph is originally directed but for some experiments, we add reversed edges to make the graph undirected - again for comparison with former work.
We call this graph \texttt{Twitter-sc-d} and \texttt{Twitter-sc-u} for the directed respectively undirected variant.

The \textit{LiveJournal} graph represents the friendship relationship of a medium sized social network.

\subsubsection{Graph patterns}

In this section, we detail the graph patterns used throughout our experiments.
Most of the queries are cyclic because that has been shown to be the primary use-case for WCOJ in former research~\cite{olddog,myria-detailed}.
WCOJ's also have been successfully applied to selective path queries in~\cite{olddog}; however, this result have not been reproduced by any
other paper.

To most of our queries, we apply a filter to make them more realistic, e.g. a clique query does make more sense if it is combined with a
smaller-than filter, which requires that the attributes are bound such that \textit{a} smaller than \textit{b}, smaller than \textit{c}.
Because otherwise, one gets the same clique in all possible orders in the output, which not only takes much more time but is also most
likely not the result a user would want.
We ensure that filters can be pushed down through or in the join by Spark as well as by the WCOJ to compare both algorithms on an equal basis.
A complete list of all queries and filters used is shown in~\cref{table:patterns}.
The less known queries are also detailed in text.
Patterns and filters might be used in all possible combinations, we name the resulting query \textit{\textless
pattern\textgreater-\textless filter\textgreater}, e.g. \textit{triangle-lt}.

\begin{table}[]
    \centering
    \begin{tabular}{@{}lcccp{6cm}@{}}
        \toprule
        Name     & Parameters                 & Vertices & Edges

        \\ \midrule
        \texttt{triangle} & NA                          & $3$        & 3                 \\
        \texttt{n-clique} & \# vertices                & $n$        & $1/2 \times n \times (n - 1)$ \\
        \texttt{n-cycle}  & \# vertices                & $n$        & $n$                 \\
        \texttt{n-s-path} & \# edges / selectivity  & $n$       & $n - 1$                 \\
    \texttt{kite}  & NA                         & $4$        & $5$                    \\
        \texttt{house}    & NA                         & $5$        & $9$                 \\
        \texttt{diamond}  & NA                         & $4$        & $4$                 \\
        \textbf{Filters}   &                            &          &                   \\
        \texttt{distinct}  &                            &          &                   \\
        \texttt{less-than} &                            &          &                   \\ \bottomrule
    \end{tabular}
    \caption{Summary of patterns and filters used.}
    \label{table:patterns}
\end{table}

\begin{figure}
    \centering
    \subfloat[triangle]{\includesvg[width=0.2\textwidth]{triangle}}
    \subfloat[4-clique]{\includesvg[width=0.2\textwidth]{4clique}}
    \subfloat[5-clique]{\includesvg[width=0.2\textwidth]{5clique}}
    \subfloat[4-cycle]{\includesvg[width=0.2\textwidth]{4cycle}}\\
    \subfloat[5-cycle]{\includesvg[width=0.2\textwidth]{5cycle}}
    \subfloat[diamond]{\includesvg[width=0.2\textwidth]{diamond}}
    \subfloat[kite]{\includesvg[width=0.2\textwidth]{kite}}
    \caption{Queries used in our experiments.}
\end{figure}


% TODO add kite query
For a selective path query, we first select two sets of nodes with respect to the \textit{selectivity} parameter.
Then we search for all path of a certain length according to the \textit{edges} parameter, e.g. \texttt{4-0.1-path} finds all
paths between two randomly selected, fixed sets of vertices of length 4 - the sets of nodes contain roughly 10\% of all input nodes and are not guaranteed to be intersection free.
% TODO do I want them to be intersection free?

\subsection{Baseline: \texttt{BroadcastHashJoin} vs \texttt{seq}}

% TODO use uniform spelling for broadcasthashjoin
In this experiment, we compare the runtime of our sequential Leapfrog Triejoin implementation, \texttt{seq}, with the runtime of Spark's \texttt{BroadcastHashjoin}.
Towards, this goal we ran all queries from~\cref{table:patterns} on our three main datasets: \texttt{ama-0302}, \texttt{ama-0601} and \texttt{snb-sf1}.
The clique patterns are combined with the less-than filter, \texttt{n-clique-lt}, and the cycle pattern with the distinct filter,
\texttt{n-cycle-distinct}.
These seem to be the most realistic setups because cliques are fully symmetric and one wants to avoid redundant results.
For cycles, the less-than filter is too restrictive because it excludes cycles for which $a < b > c$.
We show our results in~\cref{table:seq-vs-bhj} and in barcharts (\cref{fig:seq-bar-ama-0302}, \ref{fig:seq-bar-ama-0601} and \ref{fig:seq-bar-snb-sf1}).
\Cref{sssec:seq-analysis} analyzes the results.

Our experiment measures the time it takes to perform a \texttt{count} on the cached dataset using \texttt{BroadcastHashjoin} and \texttt{seq}.
For \texttt{BroadcastHashjoin}, the time to run the whole query is reported.
For \texttt{seq}, we report setup time and the time, it takes to run the join, separately.
Setup time includes the sorting, materialization and copying the results of our join from a Scala \texttt{Array} into the \texttt{UnsafeInternalRow} format
expected by Spark.  TODO sorting not yet (data is presorted in files)
This section is focused on comparing the runtimes excluding the setup time - rational given in~\cref{sssec:seq-experiment-rational}.

\subsubsection{Experiment Rationale}\label{sssec:seq-experiment-rational}
\textbf{Question:} Why do we compare against Spark's \texttt{BroadcastHashjoin} instead of \texttt{SortMergeJoin}? \\
\textbf{Answer:} Because even when all data is arranged in a single partition, for simple sequential processing, Spark
schedules its \texttt{SortMergeJoin} to use a shuffle.
A shuffle writes and reads data to and from disk.
Hence, \texttt{SortMergeJoin} is much slower than a \texttt{BroadcastHashJoin}.
We compared the algorithms on the \texttt{Amazon-0601} dataset for the \texttt{triangle} (8.1 seconds vs 58.9 seconds) and
\texttt{5-clique} pattern (32.9 seconds vs 850.9 seconds).
We assume that Spark is able to optimize its broadcasts when \texttt{local[1]} is used to start the Spark session because then Spark uses the driver as executor.

\textbf{Question:} Why do we exclude setup times from the WCOJ times?\\
\textbf{Answer:} Because our final implementation \texttt{dist} is meant to cache the readily sorted and formatted edge tables and reuse it for multiple queries.
We anticipate that this is necessary to benefit from WCOJ's in general.
Furthermore, we optimize the setup times in later implementation.
Hence, the current setup code is much slower than the one we expect to use for later implementations.
% TODO can I write that this is in line with former research?

\textbf{Question:} Why is the time to copy results into \texttt{UnsafeInternalRow} format for WCOJ counted as setup time?\\
\textbf{Answer:} It is time solely spent for integration with Spark and not Leapfrog Triejoin specific.
Furthermore, it could be avoided by working directly on the \texttt{UnsafeInternalRow} format within our \texttt{seq} implementation.
However, this would require us to work with unmanaged memory (the \texttt{UnsafeInternalRow} interface is slower than working on \texttt{Arrays})
and we deem this as an unnecessary engineering overhead.

\textbf{Question:} Is Spark's code generation a huge advantage for the \texttt{BroadcastHashjoin}?
\textbf{Answer:} Yes, we ran Spark without code generation for comparision on the \texttt{Amazon-0302} dataset for \texttt{triangle} and
\texttt{5-clique}: with code generation Spark takes 3.1 and 4.2 seconds without 14 and 16.

\subsubsection{Analysis}\label{sssec:seq-analysis}
For now, we settle to simply point out the most important observations and postpone deeper analysis, e.g. influence of dataset size and characteristics,
to experiments run against our later implementations, i.e. \texttt{seq-graph-pattern} and \texttt{dist}.

We are able to beat Spark's \texttt{BroadcastHashjoin} on all datasets and queries except \texttt{5-clique-lt} on \texttt{Amazon-0602}.
Generally, we see that for \texttt{n-clique} patterns the speedup over Spark decreases for bigger $n$.
This is due to the fact that many binary joins in a \texttt{n-clique} are actually semi-joins which to do not increase but decrease the size of intermediary results,
e.g. for \texttt{5-clique} on \texttt{Amazon-0302} only 3 out of 9 joins lead to a bigger intermediary result.

The cycle query results are highly interesting because we see an increasing speedup for higher $n$ on \texttt{Amazon-0602} but a
decreasing speedup on \texttt{Amazon-0302}.
Unfortunately, we are (TODO currently) not able to provide \texttt{n-cycle} results for the \texttt{SNB-sf1} dataset, due to the fact that
\texttt{BroadcastHashjoin}'s take more than 22 hours for the \texttt{6-cycle} which blocked our experiments.
\texttt{5-cycle} runs at the moment.

The \texttt{House} and \texttt{5-clique} pattern seem to be quite similiar - the \texttt{House} is a \texttt{5-clique} with two missing
edges.
However, as the count of their results indicates these two edges lead to dramatically different outcomes.
Hence, their different timing and speedup behaviour.

The \texttt{Kite} pattern produces consistently the second highest speedup after the \texttt{3-clique}.
Most likely due to the fact that a \texttt{Kite} is two triangles back-to-back.

The path query shows very different behaviour on the \texttt{Amazon} and the \texttt{SNB} datasets.
This might be due to the different selectivity; it is extremely high on the co-purchase datasets and rather low on the social network
benchmark.
This different in selectivity is not surprising given that the \texttt{SNB} network fulfills the small world property, while the
\texttt{Amazon} dataset relates products purchased together which naturally leads to multiple loosely connected, denser components.
We will run the path queries with a different selectivity on the two input vertice sets to confirm this hypothesis.

Finally, we observe that all three datasets lead to quite different results which are most likely not comparable to each other without deeper research
in the characteristics of the datasets themselves.
In particular, it becomes clear that co-purchase datasets and social network datasets must have very different characteristics.
Although, \texttt{SNB-sf1} is much smaller than \texttt{Amazon-0601}, queries on it take a similar or even much more time,
e.g. \texttt{5-clique-lt} takes 14.21 seconds on the bigger dataset and 12.65 seconds smaller, even though, the result set is much
smaller on \texttt{SNB-sf1};
\texttt{4-cycles-distinct} takes roughly 8 times longer on the small dataset and has a much bigger result set.
In general, we see a higher speedup on \texttt{SNB-sf1}
% TODO include filter names in query names
\begin{figure}
    \centering
    \includesvg{seq-bar-ama0302}
    \caption{\texttt{seq} vs \texttt{BroadcastHashJoin} on \texttt{Amazon-0302}}
    \label{fig:seq-bar-ama-0302}
\end{figure}

\begin{figure}
    \centering
    \includesvg{seq-bar-ama0601}
    \caption{\texttt{seq} vs \texttt{BroadcastHashJoin} on \texttt{Amazon-0601}}
    \label{fig:seq-bar-ama-0601}
\end{figure}

\begin{figure}
    \centering
    \includesvg{seq-bar-snb-sf1}
    \caption{\texttt{seq} vs \texttt{BroadcastHashJoin} on \texttt{SNB-sf1}}
    \label{fig:seq-bar-snb-sf1}
\end{figure}

\begin{table}
    \centering

    \input{generated/seq-table-ama0302}
    \vspace{0.3cm}

    \input{generated/seq-table-ama0601}

    \vspace{0.3cm}
    \input{generated/seq-table-snb-sf1}
    \caption{Runtimes for \texttt{BroadcastHashJoin} and \texttt{seq}.
    The speedup is calculated between join times and excludes setup.
    From top to bottom for dataset: \texttt{ama-0302}, \texttt{ama-0601} and \texttt{snb-sf1}.
    All times in seconds.
    }
    \label{table:seq-vs-bhj}
    % TODO align and remove headers
\end{table}


\subsection{Scaling of \textit{Graph\textsc{WCOJ}}} \label{subsec:scaling-graphWCOJ}

In this section, we aim to analyse and compare the scaling of \textit{Graph\textsc{WCOJ}} using different
partitioning schemes.
Towards this goal, we run \textit{Graph\textsc{WCOJ}} on datasets of different size namely Twitter,
LiveJournal and Orkut.
We compare two partitioning schemes: Shares and \textit{work-stealing}.
These are the two most promising schemed identified in \cref{ssec:partitionings}.
The experiment is performed on 3-clique and 5-clique.
3-clique is the smallest of our queries.
Therefore, it is most difficult to scale.
5-clique takes much longer than 3-clique.
Hence, it shows how query size influences the scaling.
Also, it increases the job size for the \textit{work-stealing} partitioning scheme.

% TODO paragraph about section structure

\subsubsection{Results}

\begin{figure}
    \centering
    \subfloat[Twitter dataset\label{fig:graphWCOJ-scaling-twitter}]
      {\includesvg[width=0.5\textwidth]{graphWCOJ-scaling-twitter}}
    \subfloat[LiveJournal dataset\label{fig:graphWCOJ-scaling-livej}]
      {\includesvg[width=0.5\textwidth]{graphWCOJ-scaling-livej}}
    \newline
    \subfloat[Orkut dataset\label{fig:graphWCOJ-scaling-orkut}]
      {\includesvg[width=0.5\textwidth]{graphWCOJ-scaling-orkut}}
    \caption{Scaling behaviour of Shares and work-stealing on three different datasets
      and two different queries.
      The batch size parameter for \textit{work-stealing} is chosen for balance between lock contention and worker skew:
      50 for Twitter and 3-clique on LiveJournal, 1 for 5-clique on LiveJournal and 20 on the Orkut dataset.
      Measurements for 5-clique for low levels of parallelism are missing for LiveJournal and Orkut due to the time it would take
      to collect the results.
    }
    \label{fig:graphWCOJ-scaling}
\end{figure}

We first describe our expectations of the experiment outcome.
We assume that scaling improves with the dataset size.
Hence, we should see the highest speedups for Orkut, then LiveJournal and the lowest speedups for Twitter.
Also, we expect the scaling to improve with the query size.
Both hypothesis are grounded in the fact that more work to distribute often leads to stronger scaling.
Additionally, we believe that \textit{work-stealing} shows better scaling than Shares because it does not duplicate work.
Finally, we have no clear cut expectations to the scaling behaviour of \textit{work-stealing}.
Theoretically, we could expect linear scaling for it because no work is duplicated, synchronization overhead is minimal and
work balance should be given by the scheme.
However, we measure on a quite complex hardware platform which complicates scaling behaviour.

First of all, we work on a machine with 4 sockets.
This can influence scaling positively and negatively.
Positively because adding more sockets means to add significantly more L3 cache (30 MB shared per socket).
If we do not use all cores on a socket, each used core can use a bigger share of this cache.
Negatively because each socket is in a different NUMA zone and the graph is not guarantued to be chached in all
NUMA zone.
Indeed, Spark shares the broadcasts for all tasks on a single executor.
So there is only one copy in memory. % TODO double check

Additionally, we run on an Intel processor with hyperthreading.
Hence, we can not expect linear speedup above 48 workers because after multiple threads will share resources and cannot be
expected to reach the same performance as two cores.

To conclude, we expect sub-linear speedup for Shares and better but still sub-linear speedup for \textit{work-stealing}.
Anyhow, it super-linear scaling in MapReduce like systems is not unheard of and could be possible on our machines.
% TODO cite the internet source from Monday.

We describe our observations per dataset;
starting with Twitter.
As expected, both partitioning schemes scale better when we increase the query size.
For 5-clique, \textit{work-stealing} exhibits near linear scaling up to 48 workers, while
clique-3 reaches the maximum speedup of 6.22 for 8 workers.
The highest speedup for 5-clique is 45 on 96 workers; clique-3 reaches its highest speedup
with 13.2 on 64 workers.
Shares lacks behind in scaling for both queries and all levels of parallelism.
The best observed speedup is 21.3 for 5-clique and 96 workers.
% TODO spark overhead
% TODO bad scaling for hyperthreading

The experiment on LiveJournal confirms our hypothesis that bigger datasets lead to better
speedups;
the highest observed speedup is 61.2 for \textit{work-stealing} on 3-clique and
36.81 for Shares on 5-clique each with 96 workers.
Also, we can confirm that Shares scales better on 5-clique than on 3-clique; with the exception of 32 workers.
However, this is not the case for \textit{work-stealing}.
\textit{work-stealing} shows better speedups on clique-3 than on clique-5.
Nevertheless, \textit{work-stealing} beats Shares on both queries and all levels of parallelism.
% TODO analyse, higher skew rigth, at least name reason, is skew higher because of 0th vertice?
% TODO effect of dataset size leads to 5-clique scaling the same or worse

Additionally, we see two strange scaling behaviours for LiveJournal.
First, super-linear scaling for 3-clique and \textit{work-stealing}.
We hypothize that this is the fact because if the 32 processes are distributed over all 4 sockets they share in total
120 MB of L3 cache while a single process can use only 30 MB of L3 cache.
To confirm this we rerun the experiment with 1, 8, 16 and 32 workers while using \textit{taskset} to bind the application
to the first 8, 16 or 32 cores.
This rules out the use of more than 1, 2 or 3 sockets respectively.
In this experiment, we measure speedup of 8.6, 16.6 and 32.9 for 8, 16 and 32 workers.
This is significantly lower than the speedup measurements without \textit{taskset}.
We conclude that this confirms our hypothesis and believe that the slight super-linear
scaling that remains arises from the bigger amount of L1 and L2 cache in the system.
% TODO run with perf counter
% TODO footnote taskset
% TODO note why do we have locallity in data accesses?

Second, Shares exhibits lower speedup of 12.1 for 64 workers which is
lower than for 32 workers (14.7) and 14.8 for 96 workers.
This can be explained by the chosen Shares configuration.
For 32 workers, the best configuration is given by the hypercube of the sizes 4, 4, 2.
For 64 workers, we get the hypercube with 4 workers on each axis.
Hence, although we are doubling the number of workers, we use the new workers only to partition
work along the last axis, in the case of 3-clique along the C attribute axis.
Partitioning work along the last axis leads to a high amount of duplicated work on the first
two axis.
Additionally, with 64 workers at least 12 of these workers are not exclusive cores but cores shared by
two hyperthreads.
In total, we get a lower speedup.
This changes slightly for 96 workers because the optimal hypercube configuration here is 6, 4, 4 which
adds more workers along the first axis.
However, the scaling only increases marginally by 0.1 from 32 workers which is quite disappointing given
that the number of threads increased by a threefold.

% TODO should be in partitionings instead.
One could argue that we should use a different definition of \textit{best} hypercube configuration.
As we see, it is not necessarily efficient to distribute the computation along the last axis.
We implemented version of the configuration finder that considers only the first \textit{i} axes and
call this partitioning scheme \textit{i-prefixShares}.
TODO report results
However for time constraints, we do not investigate this issue further and do not include \textit{i-prefixShares}
in our further experiments.
% TODO why?

The Orkut and LiveJournal datasets lead to highly similiar scaling results:
strong linear scaling for \textit{work-stealing} up to 32 workers,
\textit{work-stealing} scales significantly better than Shares for 3-clique but
not for 5-clique and Shares exhibits less speedup for 64 workers than for 32 and 64 workers.
% TODO run on 48 cores,
% TODO reevaluate similarity with LiveJournal

% TODO most important open question 1: Shares faster than workstealing? clique 5 bad scaling workstealing?
% TODO most important open question 2: workstealing slower on bigger queries?









\subsubsection{Analysis}


% show DAG's for both algorithms in terms of DAG
% measure if spark actually takes more than 30 minutes on long queries
% measure path queries


% measure against none code generated spark, notice in sentence
% measure against Sortmerge join, notice in sentence

% comparision against other work
%   Dewitt
%   Andreas Amler
%   Old dog
%   LFTJ
%   Richard