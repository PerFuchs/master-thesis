\section{Worst-case optimal join parallelization}  % TODO header

Based on the fact that Shares is an optimal partitioning scheme for n-ary joins in MapReduce like systems~\cite{shares} and
our analysis that Shares converges to a full broadcast of the graph edges (see \cref{shares-proof}), we decided
to forego physical partitioning of the graph.
We cache the graph in memory such that each Spark task can access the whole graph.
Then, we experiment with multiple \textit{logical} partitioning schemes which ensure that each task processes
only some parts of the graph.

This design allows us to implement a new flavour of the Shares partitioning in which we filter the vertices of the
graph on-the-fly while processing it with our \textit{Graph\textsc{WCOJ}} algorithm.
We describe this contribution in \cref{ssec:shares-logical}.

Furthermore, we consider a work-stealing based partitioning which does not replicate any work and produces less
skew than Shares. % TODO is that always correct.
This comes at the price of implementing work-stealing on Spark.
The design of work-stealing in Spark is described in \cref{ssec:work-stealing}

\subsection{Logical Shares} \label{ssec:shares-logical}
Shares has been developed as an optimal shuffle for n-ary joins on MapReduce like systems.
So, it is used to physical partition the tables participating in the join over all workers of the system.
Then, each worker works only on the tuples it holds in its partition.
This has been implemented in Myria for \textsc{WCOJ}~\cite{myria-detailed} and for Haddoop~\cite{TODO}.
We describe the Shares and Myria in more detail in~\cref{ssec:myria} and assume that the reader is familiar
with this section.

The idea of Shares can also be used for a \textit{logical} partitioning scheme.
Instead, of partitioning the graph before computing the join, we determine if a tuple should be considered by the
join on-the-fly.
We do so by assigning a coordinate of a hypercube to each worker as in the original Shares.
Then each worker is responsible for the tuples which match its coordinate.
However, in the \textsc{LFTJ} we do not consider whole tuples but only single attributes of a tuple at the time,
e.g. a \textit{LeapfrogJoin} only considers one attribute and cannot determine the whole tuple to which this attribute
belongs.
Fortunately, a tuple matches a coordinate if all hashes match the coordinate.
Hence, we can filter out a tuple if any of its attributes do not match.
For example, we can exclude a value in a \textit{LeapfrogJoin} without knowing the whole tuple.

Integrating Shares and \textsc{LFTJ} comes with two important design decisions.
First, the \textit{LeapfrogTriejoin} operates on all tuples.
Hence, we need to filter out the values that do not match the coordinate of the worker.
Second, where to compute the optimal Hypercube configuration.
We describe our solutions below.

% TODO main difference between precomputed filtered relationship and one shared on-the-fly filtered relationship.

The first design decision is where to filter the values.
The \textit{LeapfrogTriejoin} consists out of multiple components which are composed as layers upon each other.
On top we have the \textit{LeapfrogTriejoin} which operates on one \textit{LeapfrogJoin} per attribute.
The \textit{LeapfrogJoins} uses multiple \textit{TrieIterators}.
Our first instinct is to push the filter as deep as possible into these layers.

We built a \textit{TrieIterator} that never returns a value which hash does not match the coordinate.
This is implemented by changing the \textit{next} and \textit{seek} methods such that they linearly
consider further values until they find a matching value.
However, the resulting \textsc{LFTJ} was so slow that we abandoned this idea immediately.
We hypothesize that this is the case because the original \textit{next} and \textit{seek} method is now followed
by a linear search for a matching value.
Furthermore, many of these values are later dropped in the intersection of the \textit{LeapfrogJoin} which
can also be seen as a filter over the values of the \textit{TrieIterators}.
As we know from~\cref{ssec:leapfrog-materialization}, the \textit{LeapfrogJoin} is a rather selective filter.
It does not make sense to push a less selective filter below a more selective filter.

With this idea in mind, we build a logical Shares implementation that filters the return values of the \textit{leapfrogNext}
method.
This is implemented as a decorator pattern around the original \textit{LeapfrogJoin}.
The use of the decorator pattern allows us to easily integrate Shares while at the same time allows for
other partitioning schemes later on.

% TODO to negative?
The second design decision is how and where to compute the best hypercube configuration.
The how has been discussed extensively in former literature~\cite{shares,myria-detailed}
% TODO there are one or two more papers, see myriad-detailed
We implement the exhaustive search algorithm used in the Myria system~\cite{myria-detailed}.
%In their paper they conclude that this is a `practical and efficient' solution.
%Given that the computation of the hypercube configuration can cost up to 45s for 96 workers, this is only
%the case for quite long-running queries.

In the interest of a simple solution, we compute the best configuration on the master before starting the Spark
tasks for the join.
We note that the exhaustive algorithm could be optimized easily and it would be worthwhile to introduce
a cache for common configurations.
Due to time constraints, we leave this to future work and keep our focus on the scaling behaviour of Shares.

To conclude, we succeeded to integrate Shares with \textit{LeapfrogTriejoin} and report our results in \cref{ssec:graphWCOJ-scaling}.
We cannot improve on the main weakness of Shares that it duplicates a lot of work.
Indeed, our design filters out tuples only after the \textit{LeapfrogJoin}.
Therefore, all tuples are considered in the \textit{TrieIterator} and their binary search.
However, we can share the same CSR data structure for all \textit{TrieIterator} and do not
need to materialize a prefiltered data structure for each \textit{TrieIterator} which saves time and memory.

\subsubsection{RangeShares}
In the last section, we raised the point that our Shares implementation only filters out value after the
\textit{LeapfrogJoins}.
This is because the hash-based filter of Shares filters values one-by-one.
In this section, we explore the possibility to use range based filters which can be pushed into the \textit{TrieIterators}.
However, we warn the reader that this is a negative result.
It leads to high skew which hinders good scaling of this idea.

We observe that the general idea behind Shares is to introduce a mapping per attribute from the value space into the space of possible
hypercube coordinates, e.g. so far all Shares variants use a hash function per attribute to map the values onto the hypercube.
We investigate the possibility to use ranges as mapping function, e.g. in a three-dimensional hypercube with three workers per dimension,
we could divide the value space into three ranges then the a value matches a coordinate if it is in the correct range.
In contrast to hash-based mappings, we can push a range based mapping into the \textit{TrieIterators}.
They can check after each call to \textit{seek} and \textit{next} if they are still in their range.
Contrary, to hash-based mappings which are checked value by value until one matches, a range check is a single conditional after each
function call.
Furthermore, this conditional is predictable for the processor: for all but one call, the value is in range and returned.

We implement this idea by dividing the vertice ids per attribute into as many ranges as the size of the corresponding hypercube dimension.
For example, assume we have edge ids from 0 to 899, three attributes and the hypercube dimension have the size 3, 2 and 2.
Then, we choose the ranges [0,300), [300,600) and [600,900) for the first attribute and the ranges [0,450) and [450,900) for the other
two attributes.
The worker with the coordinate (0, 0, 0) is then assigned the ranges [0,300), [0,450) and [0,450).
It configures its \textit{TrieIterators} accordingly such that they are limited to these ranges.

We run first experiments to evaluate this idea.
We expect it to scale better than a hash-based Shares because it saves intersection work in the \textit{LeapfrogJoins}.
However, we find that high skew between the workers leads to much worse performance than a hash-based Shares.
% TODO present data?
The explanation is that if a worker is assigned the same range multiple times and this range turns out to take long to compute, it takes
much longer than all other workers.

To mitigate this problem, we break down the vertice ids into more ranges than there are workers in the hypercube dimension corresponding to
the attributes.
Then, we assign multiple ranges to each \textit{TrieIterator} in such a way that the overlap on the first two attributes equals the overlap
of a hash-based implementation for the first two attributes and assign the ranges of the later attributes randomly.
However, experiments still show a high skew: some workers find many more instances of the searched pattern in their ranges than others.
For the triangle query on \textit{LiveJournal}, we find that the fastest worker outputs only 0.4 times the triangles than the slowest worker.
We conclude that the pattern instances are unevenly distributed over the ranges of vertice ids which leads to high skew in
a range based solution.
We stopped our investigation in this direction.

% TODO source code for clarification?


\subsection{Work-stealing} \label{ssec:work-stealing}
