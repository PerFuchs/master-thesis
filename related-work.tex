\section{Related Work} \label{sec:related-work}


\subsection{Graphs on Spark}
Due to its generality, widespread acceptance in the industry, the ability to use cloud hardware and its fault tolerance by design, it is an attractive target for big graph processing.
For example, GraphFrames~\cite{graphframe}, GraphX~\cite{graphx} (a Pregel~\cite{pregel} implementation) or graph query languages as \mbox{G-CORE}~\cite{gcore} and \mbox{openCypher} with their `Cyper for Apache Spark'~\cite{caps} all aim to ease graph processing on Spark.
The last two technologies translate their graph specific operations to the relational interface of Spark (SparkSQL) to profit from Spark's relational query optimizer Catalyst~\cite{spark-sql}.
Hence, we believe that the WCOJ's, with their efficiency for analytical graph queries, are a valuable addition to Spark's built-in join algorithms in general and these graph-on-spark systems in particular.

% TODO eventually include binary search papers

\subsubsection{Fractal a graph pattern mining system on Spark}

%However, it has been shown that one can use multiple rounds of Shares to trade-off more shuffle operations for less replication while maintaining optimal communication costs~\cite{shares-multi-round}.
%Lately, in 2018, Afrati et al. published a Shares version which can handle skew and implemented it in Hadoop~\cite{sharesSkew,hadoop}.
\subsection{\textsc{WCOJ} on Timely Data Flow}
A second distributed version of worst-case optimal joins was published in 2018 based on a Timely Data Flow system~\cite{ammar2018distributed,naiad}.
In Timely Data Flow, shuffling is a streaming, asynchronous operation which requires no disk access\footnote{The most commonly used cluster computing engines (Hadoop and Spark) implement shuffling as a synchronizing operation that requires to write and read all tuples from disk.}.
Therefore, the number of shuffle operations is less important than in Hadoop or Spark.
The authors take advantage of this fact by using a uniform, non-replicating partitioning scheme for their relationships.
Then they implement a worst-case optimal join using distributed data flow operators~\cite{naiad}, e.g. min and intersection.
Similar to us, the authors focus on scalability and efficiency in their work but due to the use of a streaming, in memory shuffles and a fine-grained batched processing scheme their approach is unlikely to be successful in Spark.
Hence, their and our research share the same goals, however, we aim to achieve it in a more restrictive, but widely used and industrial accepted, system.

% TODO longbins paper

%Below we analyse the scalability of \texttt{HC} in the context of analytical graph queries.
%First, we point out some simple implications of our use-case, then we analyse how a growing number of variables influences the number of tuples per server, the number of servers needed to build the hypercube and the number of replicated tuples in the system.

%We are discussing multi-join processing in the context of graph pattern matching; this implies that variables correspond to the number of nodes in the pattern and relations to the number of edges.
%Hence, we have at least as many relations in each join as we have variables, so the pattern is connected, e.g. as many relations as variables for a path query or $n * (n - 1) / 2$ relations for a n-clique.
%Another implication is that the join ranges only over a single relationship: the edge relationship of the graph.
%When all input relationships are of the same size, the optimal choice for shares in \texttt{HC} are $p_1 = ... = p_{A-1}$~\cite{myria-detailed}.
%In the following, we call this number the size $s$ of the hypercube.
%The third ramification of our use-case is that each relationship has exactly two attributes.
%This means each tuple has two fixed components in its coordinate while the rest is unbounded.
%In the next paragraph, we show how this knowledge helps us to understand how many tuples are sent to each node.


%Each node receives $R * |E| / s^2$ tuples with $R$ the number of relationships in the join.
%Our argument is that each relationship $r_i$ is divided onto $s^2$ nodes (the workers that form the planes of its two attributes) and all $r_i$ are of size $|E|$.
%If a node holds $|E|$ or more tuples after the shuffle, it would have been as good or better to broadcast \textit{E} to all nodes.
%Hence, $s^2$ needs to be bigger than \textit{R} and therefore bigger than \textit{A} (see implications above) to beat the communication costs of a shuffle in our case.
% Extent to unique tuples only





%A novel development in Spark is the ability to generate code to execute queries on the fly, called WholeStage codegeneration, based on a technique used in the Hyper database~\cite{hyper,jira-whole-stage,1m-rows-laptop}.
%Compiled queries have been shown to be multiple magnitudes faster than interpreted queries traditionally used in most database systems.
%Interpreted queries are most commonly implemented using the Volcano model~\cite{volcano}.
%This model provides a simple and composable interface for algebraic operators; basically every operator would provide an iterator interface.
%This interface would be used by a query by calling next on the root operator, who in turn calls next on each of its children and so on, until the next calls reach the scanning operators at the bottom of the query execution tree. 
%These would provide a single tuple which would then be "pulled" upwards through the query tree and processed by all operators. 
%When it reaches the root operator, the result is delivered to the user. 
%This happens for every tuple; hence, the approach can be described as tuple-at-a-time. % TODO wording
%Although, this interface is simple yet powerful due to its composability, it is also quite computation intensive mainly due to the high number of calls to the next function, which is often a virtual function call.
%This high number of virtual function call is not only CPU intensive but also makes bad use of CPU registers (they are spilled on every function call) and hinders compiler optimizations.
%Compiled queries avoid these costs by generating code specific to each query consisting mainly out of multiple, tight for-loops following each other.
%This speeds up processing by keeping data in the CPU registers as long as possible and avoiding materilization and function calls.
%Furthermore, it allows compiler optimizations, such as loop unrolling or ~\cite{hyper}% TODO one more.
%We are not aware of any published efforts to speed up worst-case optimal joins via code generation.

%We aim to combine the research on worst-case optimal join algorithm and Spark's extensible optimizer Catalyst to speed up graph processing for all Spark users.
%In particular, this work will be based on either of the two distributed versions of worst-case optimal join algorithms mentioned above. 
%We hope to further their work by evaluating which approach (shuffle + local join or timely data flow) works best on a MapReduce based processing engine as well as proving that worst-case optimal join algorithms
%can improve performance on a complex, optimized, existing platform that has not been built with them in mind originally, albeit their high additional cost (e.g. for sorting and need for special data structures).
