\section{Spark integration}\label{sec:spark-integration}

\subsection{User interface} \label{ssec:user-interface}
\begin{listing}[H]
    \inputminted{scala}{code/usage-example.scala}
    \caption{Example usage of a WCOJ to find triangles in graph.}
    \label{lst:usage-example}
\end{listing}

As one can see in line 16 % LINE
of~\cref{lst:usage-example}, we support a clean and precise DSL to match patterns in graphs.
This DSL is inspired by GraphFrames~\cite{graphframe}.
The user can define a pattern by its edges, each edge is written as \textit{(a) - [] -> (b)} where \textit{a} is the
source vertice and \textit{b} is the destination, multiple edges are separated by a semicolon.
A connected pattern is expressed by defining multiple edges with the same source or destination.
One should be aware, that a named source or destination is not guaranteed to be a distinct element in the graph,
e.g. \textit{(a) - [] -> (b); (b) - [] -> (c)} could be a linear path of size two or a circle between \textit{a} and
\textit{b}; in the second case \textit{a} and \textit{c} are the same element.
The reader might wonders, why we chose to stay with the GraphFrame syntax for edges of
\textit{- [] ->}, although, we could have went with something simpler, like \textit{->}.
However, sticking to the more verbose syntax allows us to include labels inside of the squared brackets
in future extensions, e.g. for our stretch goal of integration with CAPS.

The second parameter to \textit{findPattern} allows the user to specify the variable ordering used in the WCOJ algorithm.
Furthermore, the user interface takes multiple optional arguments, e.g. to apply to common filters to the output of the result,
specify different relationships to be used as input for each edge of the pattern.
The filters are \textit{distinctFilter}, ensuring that each vertice can occur only as binding for one variable, and
\textit{smallerThanFilter} to allow only output bindings were the values decrease with regards to the specified variable ordering,
e.g. the binding \textit{[1, 2, 3]} but not \textit{[2, 1, 3]} for the triangle query above.
We experienced that these queries are typical for graph queries and that the performance greatly benefits from pushing
them into the join.
Implementing the possibility to push general filters into the join would be a valuable addition but we decided against it because
it a pure engineering task.




\subsection{Integration with Catalyst} \label{ssec:integration-with-catalyst}
We integrated our WCOJ implementation into Spark such that it can be used as function on \textit{Datasets}.
Therefore, we build all components necessary to execute a WCOJ in Spark's structured queries, provided by Catalyst, this includes a
\textit{LogicalPlan},
a \textit{Strategy} to convert this logical operator into a \textit{SparkPlan}, a physical operator to materialize
RDD's in sorted fashion into a columnar representation that supports a least upper bound operation in $\mathcal{O} (\log n)$ and
finally a general implementation of Spark's \textit{zipPartition} operation to support more than 5 children.
We explain these components below; before we outline some limitations of our integration.

% TODO is the right place?
It is not in the scope of this work to integrate WCOJ's into the SQL parser of Spark.
Hence, WCOJ can only be used by Spark's Scala functional interface and not through Spark's SQL queries.

We do not integrate it into the query optimization components of Catalyst, e.g. we do not provide rules or cost-based strategies to
decide when to use a WCOJ or a binary join.
It is up to the user to decide when to use a WCOJ or a binary join.
However, our integration allows the user to intermix these freely.
The reason for this decision is, that at time of writing no published paper existed that systematically studies which queries benefit from
WCOJ's in general, nor, does research exist that studies the combination of WCOJ's and binary joins.
Only a month after we decided on our scope for Spark integration Salihoglu et al. published an arXiv paper~\cite{mhedhbi2019} that
tackled these problems for the first time.
The lack of peer-reviewed papers and the high complexity of the arXiv paper clearly illustrate that deeper integration with
Spark's optimizer is out of scope for this thesis.

%It is established that WCOJ is amendable to cyclic queries but not how strongly to which kind of cyclic query, e.g. dense (clique) or
%sparse (cycle).
%Additionally, no runtime measurements for queries bigger than 4-clique have been published.
To allow the user to call \textit{findPattern} (as defined in~\cref{ssec:user-interface}) on a \textit{Dataset}, we added a new class
to the package \texttt{org.apache.spark.sql} called \texttt{WCOJFunctions} and an implicit conversion from \textit{Dataset} to
\texttt{WCOJFunctions}.
This function parses the pattern - the necessary code is borrowed from GraphFrame~\cite{graphframe} - into a representation of a WCOJ join,
\texttt{JoinSpecification}, and creates new \texttt{AttributeReferences} named accordingly to the join variables.

Due to our decision not to build optimizer rules to introduce WCOJ's, we have to implement a new \textit{LogicalPlan} to represent WCOJ's.
This plan has as many children as relationships joined by the WCOJ.
Its \textit{output} are the \texttt{AttributeReferences} produced by \texttt{findPattern}.

The physical operator for WCOJ's, \texttt{WCOJExec}, is an implementation of the trait \texttt{SparkPlan}.
It expects as many children as the relationships in the join;
these children have to support a Trieiterator interface as specified in~\cref{ssec:seq-implementation}.
The biggest challenge in the implementation of \texttt{WCOJExec} is that Spark does not support to zip the partitions of more than 5 RDD's.
However, to operate synchronously on the partitions of all children, we need this operation for an arbitrary number of children.
Fortunately, although, not publicly available, Spark does implement \textit{zipPartitions} in a general manner in
the abstract class \texttt{ZippedPartitionsBaseRDD}.
Hence, supporting this functionality is a simple matter of subclassing this class publicly to expose its generality to our code (see
\texttt{sparkIntegration.GeneralZippedPartitionsRDD}).

% TODO use 's everywhere for plurals
We need to materialize the input RDD's for \texttt{WCOJExec}, to support the Trieiterator interface.
This is done by \texttt{ToTrieIterableRDD}, a \texttt{SparkPlan}, with a single child.
It iterates over the child and stores all tuples in a columnar representation, a \texttt{ColumnarBatch}\footnote{To all but highly
experienced Spark developers, the construct used to allow us to create an RDD which controls how its partitions are stored might look
unintuitive.
Normally, the type parameter of the \texttt{RDD} class is used to refer to the type of its elements, e.g. Sparks Catalyst module uses
mostly \texttt{RDD[InternalRow]}.
However, it is possible - and practised in the Spark project itself - to use this type parameter to give the type of the partitions of an
RDD.
This is what we do in the class \texttt{TrieIterableRDD}, which at the same time is a subclass of \texttt{RDD[InternalRow]}, so it can be
used in Catalyst.}
\footnote{Later we noticed the RDD function \texttt{glom} which allows to operate on the partitions of RDD's an array.
However, by then we had already implemented our own solution and decided to keep it for better control.}.
Furthermore, our Trieiterator implementation demands that the tuples are sorted.
Therefore, \texttt{ToTrieIterableRDD} requires its child to be ordered using the \texttt{requiredChildOrdering} functionality of Spark.

The last component of our Spark integration is a simple \texttt{Strategy} to translate a \texttt{WCOJ} logical operator into a
\texttt{WCOJExec} physical operator.
This is a one-to-one tranlation from \texttt{WCOJ} to \texttt{WCOJExec} as an implementation of Spark's abstract class
\texttt{SparkStrategy}.
Additionally, this \texttt{Strategy} also introduces a \texttt{ToTrieIterableRDD} operation before the WCOJ for every child.

% TODO move to acknowledgements
The Spark integration of this work has been inspired by the IndexedDataframes~\cite{indexed-dataframes} work of Alex Uta and the supervisors
of this thesis.

\subsection{Operator to create a CSR from a DataFrame}\label{subsec:csr-spark}
To be defined and written.

% systems integration?
%  relationship reordering
